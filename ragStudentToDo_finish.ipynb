{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Imports"
      ],
      "metadata": {
        "id": "mlR2JuH_vvxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "4kKSiB-c84Ai"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install google-generativeai\n",
        "!pip install chromadb\n",
        "!pip install typing"
      ],
      "metadata": {
        "id": "n8Cug9oevvb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZYNfcpgTvsZM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pypdf import PdfReader\n",
        "import os\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and load PDF"
      ],
      "metadata": {
        "id": "ATPAganzwKjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pdf(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    with open(save_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "def load_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text\n",
        "    return text"
      ],
      "metadata": {
        "id": "VpBgUKEGv0zG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ToDo:\n",
        "- Text splitting\n",
        "- ChromaDB\n",
        "- Prompt Construction"
      ],
      "metadata": {
        "id": "YIECEUcewUML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions"
      ],
      "metadata": {
        "id": "_BYNgGBTFwsn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Students implement text splitting function\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "def split_text(text):\n",
        "    \"\"\"\n",
        "    Split the input text into meaningful chunks.\n",
        "    Returns a list of text chunks.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# Custom embedding function using Gemini API\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        model = \"models/embedding-001\"\n",
        "        title = \"Custom query\"\n",
        "        return genai.embed_content(model=model, content=input, task_type=\"retrieval_document\", title=title)[\"embedding\"]\n",
        "\n",
        "# TODO: Students implement ChromaDB creation and querying\n",
        "def create_chroma_db(documents: List[str], path: str, name: str):\n",
        "    \"\"\"\n",
        "    Create a ChromaDB collection with the provided documents.\n",
        "    Returns the database instance and name.\n",
        "\n",
        "    Hint: Use the following to create the client:\n",
        "    client = chromadb.Client(Settings(\n",
        "        chroma_db_impl=\"duckdb+parquet\",\n",
        "        persist_directory=path\n",
        "    ))\n",
        "    \"\"\"\n",
        "    chroma_client = chromadb.PersistentClient(path=path)\n",
        "    import uuid\n",
        "    collection_name = name or f\"collection_{uuid.uuid4().hex[:8]}\"\n",
        "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
        "    collection.upsert(documents=documents,ids=[f\"id{i}\" for i in range(len(documents))])\n",
        "\n",
        "    return collection\n",
        "\n",
        "def get_relevant_passage(query: str, db, n_results: int):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant passages for the given query.\n",
        "    Returns a list of relevant text passages.\n",
        "    \"\"\"\n",
        "    results = db.query(\n",
        "    query_texts=[query], # Chroma will embed this for you\n",
        "    n_results=2 # how many results to return\n",
        ")\n",
        "    return results\n",
        "\n",
        "# TODO: Students implement prompt construction\n",
        "def make_rag_prompt(query: str, relevant_passage: str):\n",
        "    \"\"\"\n",
        "    Construct a prompt for the generation model using the query and retrieved passage.\n",
        "    Returns the formatted prompt string.\n",
        "    \"\"\"\n",
        "    prompt = f\"Given the query: '{query}', and the relevant passage: '{relevant_passage}', generate a response that answers the query based on the passage.\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "fSoJUWJiv3um"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Response Generation"
      ],
      "metadata": {
        "id": "YSO0EfflwBUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(prompt: str):\n",
        "    \"\"\"Generate answer using Gemini Pro API\"\"\"\n",
        "    gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "    if not gemini_api_key:\n",
        "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
        "    genai.configure(api_key=gemini_api_key)\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    result = model.generate_content(prompt)\n",
        "    return result.text"
      ],
      "metadata": {
        "id": "toYV28pPv_hj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main execution\n",
        "## ToDo:\n",
        " - Chat history\n",
        " - Multiple file injest"
      ],
      "metadata": {
        "id": "4LyBBDZfwlzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Set up configurations\n",
        "    pdf_url = \"https://services.google.com/fh/files/misc/ai_adoption_framework_whitepaper.pdf\"\n",
        "    pdf_path = \"ai_adoption_framework_whitepaper.pdf\"\n",
        "    db_folder = \"chroma_db\"\n",
        "    db_name = \"rag_experiment_g\"\n",
        "\n",
        "    # Create database directory\n",
        "    if not os.path.exists(db_folder):\n",
        "        os.makedirs(db_folder)\n",
        "\n",
        "    # Download and process PDF\n",
        "    download_pdf(pdf_url, pdf_path)\n",
        "    pdf_text = load_pdf(pdf_path)\n",
        "\n",
        "    # Split text into chunks\n",
        "    chunked_text = split_text(pdf_text)\n",
        "\n",
        "    # Create and set up database\n",
        "    db_path = os.path.join(os.getcwd(), db_folder)\n",
        "    db = create_chroma_db(chunked_text, db_path, db_name)\n",
        "\n",
        "    # Process user query\n",
        "    query = input(\"Please enter your query: \")\n",
        "    relevant_text = get_relevant_passage(query, db, n_results=3)\n",
        "\n",
        "    # Generate and display answer\n",
        "    if relevant_text:\n",
        "        final_prompt = make_rag_prompt(query, \"\".join(relevant_text['documents'][0]))\n",
        "        answer = generate_answer(final_prompt)\n",
        "        print(\"\\nGenerated Answer:\", answer)\n",
        "    else:\n",
        "        print(\"No relevant information found for the given query.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "VwaM3J70v1IM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "f1ee14c9-130c-4848-9227-64e201f84714"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-5 (attachment_entry):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/debugpy/server/api.py\", line 237, in listen\n",
            "    sock, _ = endpoints_listener.accept()\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 294, in accept\n",
            "    fd, addr = self._accept()\n",
            "               ^^^^^^^^^^^^^^\n",
            "TimeoutError: timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy.py\", line 52, in attachment_entry\n",
            "    debugpy.listen(_dap_port)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/debugpy/public_api.py\", line 31, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/debugpy/server/api.py\", line 143, in debug\n",
            "    log.reraise_exception(\"{0}() failed:\", func.__name__, level=\"info\")\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/debugpy/server/api.py\", line 141, in debug\n",
            "    return func(address, settrace_kwargs, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/debugpy/server/api.py\", line 251, in listen\n",
            "    raise RuntimeError(\"timed out waiting for adapter to connect\")\n",
            "RuntimeError: timed out waiting for adapter to connect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your query: What is Tactical maturity\n",
            "\n",
            "Generated Answer: Tactical maturity refers to the stage where individual teams within an organization typically manage their own separate data islands and machine learning (ML) assets without sharing them with other teams or functions. Even within the same team, it may be difficult to reuse assets due to a lack of standardization in technology and processes.\n"
          ]
        }
      ]
    }
  ]
}